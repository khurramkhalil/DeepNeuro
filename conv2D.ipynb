{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv2D.ipynb",
      "provenance": [],
      "mount_file_id": "1CuuvkzhZZAZftRwuVWQsWOVJu8TQDoeM",
      "authorship_tag": "ABX9TyOapJ7lv+Ya37Us0+IysEM/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khurramkhalil/DeepNeuro/blob/master/conv2D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PprM8ctsbr8Z",
        "colab_type": "code",
        "outputId": "2d135614-3099-46ec-aa1d-bba7e30187ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvWhv9VuwQ_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path =  \"/content/drive/My Drive/fNIRS Nature/win6secf/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILX0wW44hOau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from collections import Counter\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import np_utils\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import np_utils\n",
        "global win\n",
        "win = 60\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztSMzww86u46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def reject_outliers(data, m = 2):\n",
        "    return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
        "\n",
        "def normalize(data):\n",
        "    sc = StandardScaler()\n",
        "    s = sc.fit_transform(data)\n",
        "    return s\n",
        "\n",
        "def minmax(data):\n",
        "    # print(data)\n",
        "    sc = MinMaxScaler()\n",
        "    s = sc.fit_transform(data.astype(np.float))\n",
        "    return s\n",
        "\n",
        "def rec_plot(s, eps = None, steps = None):\n",
        "    if eps == None: eps = 1  # (3) distance, if two values within signal are closer than \\epsilon then we draw a dot.\n",
        "    if steps == None: steps = 10 # (4) Embadding time delay\n",
        "    N = s.size\n",
        "    S = np.repeat(s[None, :], N, axis = 0)\n",
        "    Z = np.floor(np.abs(S-S.T) / eps)\n",
        "    Z[Z > steps] = steps\n",
        "    return Z\n",
        "\n",
        "def slidingWindow(sequence, winSize, step = 1):\n",
        "    \"\"\"Returns a generator that will iterate through\n",
        "    the defined chunks of input sequence.  Input sequence\n",
        "    must be iterable.\"\"\"\n",
        " \n",
        "    # Verify the inputs\n",
        "    try: it = iter(sequence)\n",
        "    except TypeError:\n",
        "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
        "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
        "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
        "    if step > winSize:\n",
        "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
        "    if winSize > len(sequence):\n",
        "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
        " \n",
        "    # Pre-compute number of chunks to emit\n",
        "    numOfChunks = int(((len(sequence) - winSize) / step) + 1)\n",
        " \n",
        "    # Do the work\n",
        "    for i in range(0, numOfChunks * step, step):\n",
        "        yield sequence[i: i + winSize]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlqdxSOQwcy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fo = glob.glob(\"/content/drive/My Drive/fNIRS Nature/o*.csv\")\n",
        "fd = glob.glob(\"/content/drive/My Drive/fNIRS Nature/d*.csv\")\n",
        "\n",
        "for i in range(len(fo)):\n",
        "    oxy = pd.read_csv(fo[i])\n",
        "    doxy = pd.read_csv(fd[i])\n",
        "    \n",
        "    oxy = oxy.iloc[:].values\n",
        "    doxy = doxy.iloc[:].values\n",
        "    \n",
        "    \n",
        "    def makeplt(data): # len(g)\n",
        "        x = data\n",
        "        v = 0\n",
        "        # win = win        # Window size\n",
        "        step = 30\n",
        "        noOfSeq = int(((len(oxy[:, 1]) - win) / step) + 1)\n",
        "        x_train = np.zeros((noOfSeq, win, 36))\n",
        "        y_train = np.zeros(noOfSeq).astype(np.ndarray)\n",
        "\n",
        "        chunks = slidingWindow(x, win, step)\n",
        "        # z = []\n",
        "        for chunk in chunks:\n",
        "            ch = chunk[:, :-1]\n",
        "            lab = chunk[: , -1]\n",
        "            x_train[v, :, :] = ch\n",
        "            b = Counter(lab)\n",
        "            c = b.most_common(1)\n",
        "            c = list(list(zip(*c))[0])\n",
        "            c = np.array(c)\n",
        "            y_train[v] = c[0]\n",
        "            v += 1\n",
        "            y_train = y_train.astype('U')\n",
        "        return x_train[:-1, :], y_train[:-1]\n",
        "\n",
        "        \n",
        "    x1, y1 = makeplt(oxy)\n",
        "    x2, y2 = makeplt(doxy)\n",
        "    \n",
        "    '''\n",
        "    For Hb) + HbR\n",
        "for i in range(0, 10):\n",
        "    \n",
        "    print(i)\n",
        "    i -= 2\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    \n",
        "    # win = win\n",
        "    ch = 36\n",
        "    \n",
        "    \n",
        "    \n",
        "    x_train = np.concatenate((x1, x2))\n",
        "    y_train = np.concatenate((y1, y2))\n",
        "    \n",
        "    \n",
        "    \n",
        "    X_train = x_train.reshape(x_train.shape[0], 1, win, ch)\n",
        "    \n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    \n",
        "    \n",
        "    labelencoder_y = LabelEncoder()\n",
        "    y_train = labelencoder_y.fit_transform(y_train)\n",
        "    \n",
        "    Y_train = np_utils.to_categorical(y_train, 4)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_train, Y_train, test_size = 0.2)\n",
        "    \n",
        "    \n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.layers import Lambda\n",
        "    from tensorflow.keras import layers, models, regularizers\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    model = models.Sequential()\n",
        "     \n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='elu', input_shape = (1, win, ch), data_format = 'channels_first'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='elu', kernel_regularizer = regularizers.l2(0.01)))\n",
        "    model.add(layers.Dropout(0.1))\n",
        "    # model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    \n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='elu', kernel_regularizer = regularizers.l2(0.001)))\n",
        "    #model.add(Lambda(lambda v: tf.cast(tf.compat.v1.spectral.fft(tf.cast(v,dtype=tf.complex64)),tf.float32)))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(64, activation='elu', kernel_regularizer = regularizers.l2(0.001)))\n",
        "    # model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(4, activation='softmax'))\n",
        "    \n",
        "    # model.add(Dense(4, activation = 'softmax'))\n",
        "    \n",
        "    model.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = 'adam', metrics = ['accuracy'])\n",
        "    \n",
        "    hist = model.fit(X_train, y_train, validation_data = (X_test, y_test),\n",
        "                     epochs = 500, batch_size = 32, shuffle = True, verbose = 0)\n",
        "    model.save_weights(path + 'secondw60_try_%d.h5'%i)  # always save your weights after training or during training\n",
        "    \n",
        "    \n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = (y_pred > 0.5)\n",
        "    \n",
        "    \n",
        "    dpi = 300\n",
        "    \n",
        "    plt.close()\n",
        "    plt.figure(1)\n",
        "    plt.plot(hist.history['accuracy'])\n",
        "    plt.plot(hist.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xlabel('Epoch')\n",
        "    # plt.xlim(0, 200)\n",
        "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "    # plt.grid()\n",
        "    plt.savefig(path + \"Accuracyw60_figure(Subject)_%d.png\"%i, dpi = dpi, bbox_inches='tight')\n",
        "    \n",
        "    #plt.show()\n",
        "    \n",
        "    # \"Loss\"\n",
        "    plt.close()\n",
        "    plt.figure(2)\n",
        "    plt.plot(hist.history['loss'])\n",
        "    plt.plot(hist.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    # plt.ylim(0, 1)\n",
        "    plt.xlabel('Epoch')\n",
        "    # plt.xlim(0, 200)\n",
        "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "    # plt.grid()\n",
        "    plt.savefig(path + \"Lossw60_figure(Subject)_%d.png\"%i, dpi = dpi, bbox_inches='tight')\n",
        "    np.save(path + 'myw60_history_%d.npy'%i, hist.history)\n",
        "\n",
        "#history=np.load('my_history.npy',allow_pickle='TRUE').item()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}